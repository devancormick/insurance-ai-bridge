# Backup Policy Configuration
# Automated backup policies for disaster recovery

apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup-hourly
  namespace: insurance-ai-bridge
spec:
  schedule: "0 * * * *"  # Every hour
  successfulJobsHistoryLimit: 24
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15
            env:
            - name: PGHOST
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: host
            - name: PGDATABASE
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: database
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: password
            - name: BACKUP_BUCKET
              value: "s3://insurance-ai-bridge-backups/hourly"
            command:
            - /bin/bash
            - -c
            - |
              BACKUP_FILE="backup-$(date +%Y%m%d-%H%M%S).sql.gz"
              pg_dump -Fc | gzip > /tmp/$BACKUP_FILE
              aws s3 cp /tmp/$BACKUP_FILE $BACKUP_BUCKET/$BACKUP_FILE
              # Cleanup old backups (keep last 24 hours)
              aws s3 ls $BACKUP_BUCKET/ | awk '$4' | while read file; do
                if [ "$(date -d "$(echo $file | cut -d' ' -f1,2)" +%s)" -lt "$(date -d '24 hours ago' +%s)" ]; then
                  aws s3 rm $BACKUP_BUCKET/$file
                fi
              done
          restartPolicy: OnFailure

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup-daily
  namespace: insurance-ai-bridge
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15
            env:
            - name: PGHOST
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: host
            - name: PGDATABASE
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: database
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: password
            - name: BACKUP_BUCKET
              value: "s3://insurance-ai-bridge-backups/daily"
            command:
            - /bin/bash
            - -c
            - |
              BACKUP_FILE="backup-$(date +%Y%m%d).sql.gz"
              pg_dump -Fc | gzip > /tmp/$BACKUP_FILE
              aws s3 cp /tmp/$BACKUP_FILE $BACKUP_BUCKET/$BACKUP_FILE --storage-class STANDARD_IA
              # Cleanup old backups (keep last 30 days)
              aws s3 ls $BACKUP_BUCKET/ | awk '$4' | while read file; do
                if [ "$(date -d "$(echo $file | cut -d' ' -f1,2)" +%s)" -lt "$(date -d '30 days ago' +%s)" ]; then
                  aws s3 rm $BACKUP_BUCKET/$file
                fi
              done
          restartPolicy: OnFailure

---
# WAL Archiving Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-wal-archiving
  namespace: insurance-ai-bridge
data:
  archive.sh: |
    #!/bin/bash
    # WAL Archiving Script for Point-in-Time Recovery
    ARCHIVE_BUCKET="s3://insurance-ai-bridge-backups/wal-archive"
    
    # Archive WAL file
    WAL_FILE=$1
    ARCHIVE_NAME=$(basename $WAL_FILE)
    
    # Upload to S3
    aws s3 cp $WAL_FILE $ARCHIVE_BUCKET/$ARCHIVE_NAME
    
    # Verify upload
    if [ $? -eq 0 ]; then
      echo "Archived $ARCHIVE_NAME successfully"
    else
      echo "Failed to archive $ARCHIVE_NAME" >&2
      exit 1
    fi

